---
layout: post
title: '[ML/DL] Relu with Pytorch'
subtitle: ''
categories: programing
tags: dl
comments: true
---
## Problem of Sigmoid(Vanishing Gradient)

![](/assets/img/2020-01-20-15-33-09.png)*그림.1 Sigmoid 함수*

 위와 같은 Sigmoid 함수를 네트워크의 Activation 함수로 사용했을 때에 문제점이 있다. 계산된 결과 값이 위의 파란색 영역 부근이라면 상관이 없지만 빨간색 영역이라면 문제가 생길 수 있다. 우리는 Gradient Descent 방법을 통해 우리가 원하는 값을 찾아 나간다. Gradient Descent는 기울기 구해가면서 값을 업데이트하며 극점을 찾아가게 된다. 따라서 위와 같은 Sigmoid의 빨간색 영역은 Backpropagation을 통해 Gradient를 앞단으로 전달하면서 0에 까까운 값이 곱해지면서 Loss로부터 전달되는 Gradient가 소멸되는 문제가 발생한다. 이러한 문제를 Vanishing  Gradient 라 하며, 1-Layer 정도의 네트워크는 문제가 발생하지 않을 수 있지만, 좀 더 깊은 네트워크라면 문제가 될 것이다.

## Relu
![](/assets/img/2020-01-20-15-33-28.png)*그림.2 Relu 함수*

$$f(x)=max(0,x)$$

Relu Activation의 수식은 위와 같으며, 그래프는 그림 2. 와 같다. 0 이하인 값은 기울기가 0이되며,0 보다 큰값은 기울기가 1이되는 아주 간단한 함수이다. Relu 또한 값이 0 이하인 부분에서 기울기가 0이 되기때문에 Gradient가 손실되는 문제점이 있긴하다. Pytorch에서는 다음과 같이 사용할 수 있다.

```python
x = torch.nn.relu(x)
```

## Mnist with Relu

```python
import torch
import torchvision.datasets as dsets
import torchvision.transforms as transforms
import random

device = 'cuda' if torch.cuda.is_available() else 'cpu'

# for reproducibility
random.seed(111)
torch.manual_seed(777)
if device == 'cuda':
    torch.cuda.manual_seed_all(777)
    
# parameters
learning_rate = 0.001
training_epochs = 15
batch_size = 100

# MNIST dataset
mnist_train = dsets.MNIST(root='MNIST_data/',
                          train=True,
                          transform=transforms.ToTensor(),
                          download=True)

mnist_test = dsets.MNIST(root='MNIST_data/',
                         train=False,
                         transform=transforms.ToTensor(),
                         download=True)
                         
# dataset loader
data_loader = torch.utils.data.DataLoader(dataset=mnist_train,
                                          batch_size=batch_size,
                                          shuffle=True,
                                          drop_last=True)

# nn layers
linear1 = torch.nn.Linear(784, 256, bias=True)
linear2 = torch.nn.Linear(256, 256, bias=True)
linear3 = torch.nn.Linear(256, 10, bias=True)
relu = torch.nn.ReLU()

# Initialization
torch.nn.init.normal_(linear1.weight)
torch.nn.init.normal_(linear2.weight)
torch.nn.init.normal_(linear3.weight)

# model
model = torch.nn.Sequential(linear1, relu, linear2, relu, linear3).to(device)

# define cost/loss & optimizer
criterion = torch.nn.CrossEntropyLoss().to(device)    # Softmax is internally computed.
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

total_batch = len(data_loader)
for epoch in range(training_epochs):
    avg_cost = 0

    for X, Y in data_loader:
        # reshape input image into [batch_size by 784]
        # label is not one-hot encoded
        X = X.view(-1, 28 * 28).to(device)
        Y = Y.to(device)

        optimizer.zero_grad()
        hypothesis = model(X)
        cost = criterion(hypothesis, Y)
        cost.backward()
        optimizer.step()

        avg_cost += cost / total_batch

    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))

print('Learning finished')
```

## output:

```
Epoch: 0001 cost = 129.325607300
Epoch: 0002 cost = 36.169139862
Epoch: 0003 cost = 23.025590897
Epoch: 0004 cost = 16.021036148
Epoch: 0005 cost = 11.609578133
Epoch: 0006 cost = 8.560424805
Epoch: 0007 cost = 6.369730949
Epoch: 0008 cost = 4.782918930
Epoch: 0009 cost = 3.604729652
Epoch: 0010 cost = 2.682321310
Epoch: 0011 cost = 2.086567640
Epoch: 0012 cost = 1.640438557
Epoch: 0013 cost = 1.297079921
Epoch: 0014 cost = 1.083126664
Epoch: 0015 cost = 0.751341939
Learning finished
```

>## Reference
> [1] [모두를 위한 딥러닝 시즌2 - PyTorch](https://www.edwith.org/boostcourse-dl-pytorch/joinLectures/22155)